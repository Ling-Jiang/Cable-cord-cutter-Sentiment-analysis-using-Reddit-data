{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition as the name suggests involved identification of entities like person, orgranization, location e.t.c from a particular sentence. It is a very step of any sentiment analysis as it outputs the main entity that is present in the comment.  \n",
    "For example, running Named entity recognition on a sentennce like \"Microsoft is doing great in India\" would output *Microsoft* as an organization and *India* as a location.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this part of the analysis, we have identified the entities that are present in the comments that we obtained from Reddit for cord cutters. The data is for about 4 months of data starting from June 2018 to September 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('df_end_of_preproc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting entities from the comments and storing them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def ner(s):\n",
    "    doc = nlp(s)\n",
    "    l = []\n",
    "    for ent in doc.ents:\n",
    "        k = (ent.text,ent.label_)\n",
    "        l.append(k)\n",
    "    return l\n",
    "\n",
    "\n",
    "df['entities_labels'] = df['comments'].apply(lambda x: ner(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the main idea for extracting entites is to identify the organizations and persons that are involved in the comments. This would later help us understand in the maximum mentions for each entity in the data and the sentiment towards those entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_list = list(df['comments'])\n",
    "ents = []\n",
    "labels = []\n",
    "for comm in comm_list:\n",
    "    doc = nlp(comm)\n",
    "    for ent in doc.ents:\n",
    "        ents.append(ent.text)\n",
    "        labels.append(ent.label_)\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['ents'] = ents\n",
    "temp['labels'] = labels\n",
    "\n",
    "temp1 = temp[temp['labels'].isin(['ORG'])]\n",
    "k = list(set(temp1['ents']))\n",
    "temp2 = pd.DataFrame()\n",
    "temp2['entities'] = k\n",
    "temp2.to_csv('entities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to note that although there are quite some misclassifications in terms of recognising the entities, achieving this from a pre-trained model is still satisfactory.  \n",
    "This can be further if we can train an LSTM network on our data and use that model for future purposes on the same subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
